# One of the junior DevOps team members was working on to deploy a stack on Kubernetes cluster. Somehow the pod is not coming up and its failing with some errors. We need to fix this as soon as possible. Please look into it.


There is a pod named webserver and the container under it is named as httpd-container. It is using image httpd:latest

There is a sidecar container as well named sidecar-container which is using ubuntu:latest image.

Look into the issue and fix it, make sure pod is in running state and you are able to access the app.

The error message you provided indicates that there is an issue with pulling the "httpd:latests" image. It appears that there is a typo in the image name ("latests" instead of "latest"). To fix this issue, you need to correct the image name in the pod's YAML configuration file. 

Here are the steps to correct the image name:

1. Open the YAML configuration file for the pod. You can edit the file using a text editor or use the `kubectl edit` command to edit the pod directly.

   ```bash
   kubectl edit pod webserver
   ```

2. Locate the `httpd-container` section in the YAML file, which should look something like this:

   ```yaml
   containers:
   - name: httpd-container
     image: httpd:latests
   ```

3. Correct the image name from "httpd:latests" to "httpd:latest." It should look like this:

   ```yaml
   containers:
   - name: httpd-container
     image: httpd:latest
   ```

4. Save the changes and exit the text editor if you are using it directly.

5. Apply the updated configuration to the cluster:

   ```bash
   kubectl apply -f <pod_configuration_file>.yaml
   ```

6. Monitor the pod's status to ensure it transitions to the running state:

   ```bash
   kubectl get pods
   ```

Once the pod is in the running state, you should be able to access the application without issues. If you encounter any other errors or issues, please provide more details, and I'll be happy to assist further.



```ruby
# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{},"labels":{"app":"web-app"},"name":"webserver","namespace":"default"},"spec":{"containers":[{"image":"httpd:latest","name":"httpd-container","volumeMounts":[{"mountPath":"/var/log/httpd","name":"shared-logs"}]},{"command":["sh","-c","while true; do cat /var/log/httpd/access.log /var/log/httpd/error.log; sleep 30; done"],"image":"ubuntu:latest","name":"sidecar-container","volumeMounts":[{"mountPath":"/var/log/httpd","name":"shared-logs"}]}],"volumes":[{"emptyDir":{},"name":"shared-logs"}]}}
  creationTimestamp: "2023-11-05T16:51:16Z"
  labels:
    app: web-app
  name: webserver
  namespace: default
  resourceVersion: "6601"
  uid: 24c031a8-7ec2-4712-b81b-ebe38faa4def
spec:
  containers:
  - image: httpd:latest
    imagePullPolicy: IfNotPresent
    name: httpd-container
    resources: {}
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/log/httpd
      name: shared-logs
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-q9zkm
  - while true; do cat /var/log/httpd/access.log /var/log/httpd/error.log; sleep
      30; done
    image: ubuntu:latest
    imagePullPolicy: Always
    name: sidecar-container
    resources: {}
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/log/httpd
      name: shared-logs
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-q9zkm
      readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  nodeName: kodekloud-control-plane
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: default
  serviceAccountName: default
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - emptyDir: {}
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2023-11-05T16:51:16Z"
    status: "True"
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: "2023-11-05T17:10:37Z"
    status: "True"
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: "2023-11-05T17:10:37Z"
    status: "True"
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: "2023-11-05T16:51:16Z"
    status: "True"
    type: PodScheduled
  containerStatuses:
  - containerID: containerd://2ba10829e2519017651c3d486ea13d0812c55d78b6d1cda66cb5c47196fc7205
    image: docker.io/library/httpd:latest
    imageID: docker.io/library/httpd@sha256:4e24356b4b0aa7a961e7dfb9e1e5025ca3874c532fa5d999f13f8fc33c09d1b7
```


![image](https://github.com/Althaf-official/KodeKloud_K8s_SysAdmin_level1/assets/105126131/67d9d3d8-3c27-443b-8607-9fb33806354e)




To troubleshoot and fix the issue with the failing pod in your Kubernetes cluster, follow these steps:

1. Check Pod Status:
   Start by checking the current status of the pod and the container. You can use the following command to get detailed information about the pod:

   ```bash
   kubectl describe pod webserver
   ```

   Look for events or errors that might provide insights into why the pod is failing.

2. Review Logs:
   Review the container logs to identify any specific error messages or issues that might be causing the pod to fail. You can use the following command to view the logs of the httpd-container:

   ```bash
   kubectl logs webserver -c httpd-container
   ```

   Additionally, check the logs for the sidecar-container:

   ```bash
   kubectl logs webserver -c sidecar-container
   ```

   The logs should give you more information about what's going wrong.

3. Check Resources:
   Ensure that the resources (CPU and memory limits) allocated to the pod are sufficient. Inadequate resources can cause pods to fail. You can inspect the pod's resource requests and limits using the following command:

   ```bash
   kubectl get pod webserver -o=jsonpath='{.spec.containers[*].resources}'
   ```

   Make sure the resources allocated are reasonable for the application's requirements.

4. Image Pull:
   Confirm that the required images, 'httpd:latest' and 'ubuntu:latest,' are available and can be pulled from the container registry. You can check if the images can be pulled using the following commands:

   ```bash
   kubectl describe pod webserver | grep "Failed to pull image"
   ```

   If there's an issue with image pulling, you might need to address it, such as configuring the container registry credentials.

5. Check Readiness and Liveness Probes:
   Make sure that the pod's readiness and liveness probes are correctly configured. These probes determine when a pod is ready and when it should be restarted. Check the pod's YAML configuration file for the probes.

6. Fix Issues:
   Depending on the errors and issues you find in the previous steps, you'll need to take appropriate action to fix them. It could involve adjusting resource requests/limits, updating the image name, correcting application code/configuration, or reconfiguring probes.

7. Reapply Configuration:
   If you make any changes to the pod's configuration, apply the changes using the following command:

   ```bash
   kubectl apply -f <pod_configuration_file>.yaml
   ```

8. Monitor Pod Status:
   Continuously monitor the pod's status to ensure it transitions to the running state. Use the following command:

   ```bash
   kubectl get pods
   ```

9. Access the App:
   Once the pod is running, ensure you can access the application. You might need to use a Kubernetes service to expose the application if it's not already accessible.

If the problem persists after following these steps, please provide the specific error messages and logs, which will help in diagnosing the issue more accurately.
